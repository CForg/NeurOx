/* 
        Replicates the code on Page 309 of NNFS usng NeurOx 
*/
#import "NeurOx"        //import the NeurOx package: includes Layers,Network, Optimize
#include "spiral.ox"   // the spiral generating function

enum{Nk=100,Nn=2,K=3,Nh=64}   // Hidden layer now has 64 neurons instead of 3

main() {
    decl net,batch,target,layer0,layer1,W,opt;
  	
     [target,batch] = spiral(Nk,K);
        println("R ",rows(target)," ",rows(batch));
    layer0 =       zeros(1,Nh)
            | 0.01*rann(Nn,Nh);     //stack weights under bias
    layer1 =      zeros(1,K)
            |0.01*rann(Nh,K);   
    W = vecr(layer0)|vecr(layer1);        //vectorize all parameters (will be reshaped interally)


    net = new Network(CELoss);             // set Loss as Cross Entropy (multinomial logit));
    net.AddLayers(
        new Dense(<Nn,Nh>,RecLinAct),      //add the RecLinAct layer
        new Dense(<Nh,K>,SoftAct)
        );
    net.SetBatchAndTarget(batch,target);        
    net.SetParameters(W); 
  //   opt = new BFGS(net);     MaxControl(1000,20); 
    opt = new momSGD(net,1.0,1E-3,0.5);
    opt.itmax = 10000;
    opt.iterate(&W);
    net.PREDICTING = TRUE;
    net.Obj(W);
    println("In sample Accuracy:",net.Loss.accuracy," Penalty: ",net.penalty);

    /*  Create an out-of-sample data set to validate */
     [target,batch] = spiral(Nk,K);
    net.SetBatchAndTarget(batch,target);
    net.PREDICTING = TRUE;
    net.Obj(W);
    println("Accuracy Out of Sample:",net.Loss.accuracy," Penalty: ",net.penalty);

    }

/* SHould produce this output.  

Ox 9.06 (Windows_64/Parallel) (C) J.A. Doornik, 1994-2022 (oxlang.dev)
Layers 1. Total parmams: 192
Layers 2. Total parmams: 387
Warning in SetBatchAndTarget: rows of target not equal to neurons at top layer
0 329.562 1 2.22347 0
100 226.763 0.904883 32.134 0.203126
200 206.122 0.818813 291.548 0.315222
300 158.517 0.740929 190.204 0.334897
400 123.506 0.670454 26.4806 0.0557847
500 120.095 0.606682 152.62 0.16532
600 108.204 0.548976 58.4367 0.053912
700 104.308 0.496759 34.125 0.035468
800 101.803 0.449509 54.837 0.0432595
900 98.5325 0.406753 70.9214 0.0507623
1000 94.7921 0.368063 93.4759 0.0559604
1100 94.8866 0.333054 160.395 0.0873814
1200 89.0784 0.301375 17.1326 0.0147984
1300 87.4053 0.272709 23.8122 0.0150935
1400 85.9201 0.24677 29.8525 0.0152237
1500 84.6435 0.223297 7.4889 0.00851972
1600 81.9383 0.202058 12.848 0.0301683
1700 77.3698 0.182839 7.50934 0.0118663
1800 75.3161 0.165448 4.96632 0.00872796
1900 73.4711 0.149711 5.40453 0.00808949
2000 72.1663 0.135471 6.68883 0.00756341
2100 70.9754 0.122585 4.38817 0.00580374
2200 70.1693 0.110925 3.7237 0.00471012
2300 69.5463 0.100374 4.88888 0.0040482
2400 68.9939 0.0908268 4.5189 0.00404828
2500 68.5244 0.0821876 3.53792 0.00311859
2600 68.163 0.0743701 4.73053 0.00292647
2700 67.8603 0.0672962 4.38857 0.00248204
2800 67.605 0.0608952 5.60754 0.00209819
2900 67.384 0.055103 4.14617 0.00188463
3000 67.1931 0.0498618 3.80516 0.00164373
3100 67.0275 0.045119 3.78702 0.00144954
3200 66.8833 0.0408274 3.89809 0.00128489
3300 66.7548 0.036944 3.92462 0.00113822
3400 66.6446 0.03343 3.31043 0.00106255
3500 66.5467 0.0302502 3.92622 0.00105319
3600 66.4585 0.0273729 3.93935 0.000836993
3700 66.3825 0.0247693 3.7244 0.000814236
3800 66.3102 0.0224133 3.83951 0.000697854
3900 66.2412 0.0202814 3.95852 0.000721955
4000 66.1766 0.0183523 4.55006 0.000578457
4100 66.1233 0.0166067 2.97218 0.000500728
4200 66.0783 0.0150271 3.51288 0.000487352
4300 66.0382 0.0135977 4.51671 0.000447917
4400 66.002 0.0123044 4.04954 0.000361542
4500 65.9697 0.011134 2.81307 0.000314371
4600 65.9403 0.010075 3.78811 0.000301788
4700 65.9145 0.00911666 3.9078 0.00027775
4800 65.8908 0.00824951 3.84679 0.00025074
4900 65.8698 0.00746484 3.90566 0.000209667
5000 65.851 0.0067548 3.88974 0.000188541
5100 65.834 0.0061123 3.98755 0.000182499
5200 65.8187 0.00553092 3.34907 0.000153895
5300 65.8048 0.00500483 3.97331 0.0001468
5400 65.7925 0.00452878 3.86175 0.000148115
5500 65.7811 0.00409802 2.99889 0.00012299
5600 65.7711 0.00370823 3.29339 0.000103925
5700 65.7618 0.00335551 3.75805 0.000105364
5800 65.7535 0.00303634 2.92821 9.04762e-05
5900 65.7462 0.00274753 2.98361 8.17863e-05
6000 65.7393 0.00248619 3.79713 7.84475e-05
6100 65.7332 0.00224971 3.18665 6.54083e-05
6200 65.7278 0.00203573 3.72574 5.56486e-05
6300 65.7227 0.00184209 3.76942 5.75942e-05
6400 65.7182 0.00166688 4.35985 4.96813e-05
6500 65.7142 0.00150833 3.73982 4.16417e-05
6600 65.7105 0.00136486 3.80998 4.04261e-05

Iteration complete
In sample Accuracy:0.9 Penalty: 0.870219
Warning in SetBatchAndTarget: Network already built: resizing input and output
Warning in SetBatchAndTarget: rows of target not equal to neurons at top layer
Accuracy Out of Sample:0.886667 Penalty: 0.870219
 *  Terminal will be reused by tasks, press any key to close it. 

 *  Executing task in folder NeurOx: C:\PROGRA~1\OxMetrics9\ox\oxl.exe -iC:\Users\Chris\Documents\OFFICE\software\Microeconometrics\NeurOx\include NNFS_P309 


Ox 9.06 (Windows_64/Parallel) (C) J.A. Doornik, 1994-2022 (oxlang.dev)
Layers 1. Total parmams: 192
Layers 2. Total parmams: 387
Warning in SetBatchAndTarget: rows of target not equal to neurons at top layer
0 329.561 1 2.22362 0
100 224.131 0.904883 10.3935 0.19188
200 182.148 0.818813 122.014 0.212474
300 148.08 0.740929 93.3787 0.159635
400 124.876 0.670454 41.779 0.0886759
500 121.295 0.606682 175.493 0.17061
600 107.469 0.548976 35.2198 0.0428449
700 109.515 0.496759 209.667 0.159194
800 95.2743 0.449509 155.174 0.116306
900 85.3589 0.406753 12.3318 0.0235822
1000 82.2802 0.368063 33.1123 0.0252325
1100 79.8204 0.333054 27.4167 0.0203636
1200 77.7494 0.301375 9.13144 0.0143349
1300 76.1109 0.272709 15.395 0.0144157
1400 74.7038 0.24677 7.1585 0.0101816
1500 73.5313 0.223297 7.12663 0.00951494
1600 72.5168 0.202058 5.60515 0.0075369
1700 71.6648 0.182839 7.96909 0.0075221
1800 70.8064 0.165448 5.58427 0.00647234
1900 70.0746 0.149711 5.16843 0.00594116
2000 69.479 0.135471 5.76126 0.00507886
2100 68.994 0.122585 6.75778 0.00538691
2200 68.5765 0.110925 6.0303 0.00363326
2300 68.2115 0.100374 5.96064 0.00331901
2400 67.9024 0.0908268 5.31313 0.00299101
2500 67.6244 0.0821876 4.95888 0.00299373
2600 67.3827 0.0743701 5.46329 0.00270587
2700 67.1688 0.0672962 4.95842 0.00217103
2800 66.9812 0.0608952 6.60927 0.0018338
2900 66.8181 0.055103 4.57952 0.00182313
3000 66.6088 0.0498618 4.87207 0.00215316
3100 66.3873 0.045119 5.90577 0.00161681
3200 66.2665 0.0408274 4.52666 0.00131687
3300 66.165 0.036944 5.40292 0.0012152
3400 66.0689 0.03343 5.48512 0.00111786
3500 65.9698 0.0302502 4.65689 0.000975317
3600 65.8876 0.0273729 4.71914 0.000813489
3700 65.8213 0.0247693 4.58258 0.000775981
3800 65.7643 0.0224133 5.34993 0.000741958
3900 65.7093 0.0202814 5.61473 0.000604864
4000 65.6606 0.0183523 4.66407 0.000542713
4100 65.6171 0.0166067 4.82844 0.000498764
4200 65.5788 0.0150271 4.72857 0.000600924
4300 65.5434 0.0135977 4.475 0.000406323
4400 65.5124 0.0123044 4.44426 0.000350089
4500 65.4846 0.011134 4.77217 0.000339139
4600 65.4587 0.010075 4.80605 0.000326799
4700 65.4355 0.00911666 4.73513 0.000255496
4800 65.4147 0.00824951 4.4521 0.000252782
4900 65.3965 0.00746484 4.63061 0.000209904
5000 65.3786 0.0067548 4.38561 0.00018896
5100 65.3634 0.0061123 4.48723 0.000204586
5200 65.3494 0.00553092 4.31615 0.000156975
5300 65.337 0.00500483 4.65005 0.000149172
5400 65.3259 0.00452878 4.88766 0.000133327
5500 65.3157 0.00409802 4.80168 0.000112243
5600 65.3067 0.00370823 4.64952 0.000103304
5700 65.2982 0.00335551 4.46249 9.54164e-05
5800 65.2907 0.00303634 5.53852 9.33825e-05
5900 65.2839 0.00274753 4.48066 7.88204e-05
6000 65.278 0.00248619 4.23939 6.76648e-05
6100 65.2723 0.00224971 4.39532 6.38304e-05
6200 65.2672 0.00203573 4.61354 6.4142e-05
6300 65.2626 0.00184209 4.55769 5.22794e-05
6400 65.2585 0.00166688 5.35224 5.01059e-05

Iteration complete
In sample Accuracy:0.916667 Penalty: 0
Warning in SetBatchAndTarget: Network already built: resizing input and output
Warning in SetBatchAndTarget: rows of target not equal to neurons at top layer
Accuracy Out of Sample:0.876667 Penalty: 0


*/